{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "a7f3ecc586ccb64ed2683ad4223f5732", "grade": false, "grade_id": "cell-c303019816ad7b1d", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "## Canciones en Spotify entre 1958 y 2019\n\nSpotify, en su conjunto, tiene una biblioteca de 30 millones de canciones. No solo contiene estas canciones, sino que tambi\u00e9n define y documenta los atributos de cada una de estas canciones (el tipo de atributos se aclarar\u00e1 a continuaci\u00f3n). Esta es una cantidad incre\u00edble de informaci\u00f3n que requiere una arquitectura de Big Data para ser utilizada correctamente. Y es fundamental contar con esta arquitectura porque existe un inmenso potencial para la monetizaci\u00f3n de estos datos dentro de la industria de la m\u00fasica.\n\nEl conjunto de datos final que se debe utilizar en el an\u00e1lisis contiene 277.965 filas y 30 columnas para 24.000 canciones. La clave primaria de los datos es una combinaci\u00f3n de WeekID y SongID.\n\n* url: la URL de la canci\u00f3n en el sitio de la cartelera.\n* WeekID: la semana para la que se registra el rendimiento de la canci\u00f3n\n* Week Position: la posici\u00f3n que ocup\u00f3 la canci\u00f3n durante esa semana (var\u00eda de 1 a 100)\n* Song: t\u00edtulo de la canci\u00f3n\n* Performer: nombre del int\u00e9rprete\n* SongID: identificador de Billboard para la canci\u00f3n (concatenaci\u00f3n de columnas anteriores)\n* Instance: n\u00famero de veces que se ha producido\n* Previous Week Position: posici\u00f3n de la semana pasada\n* Peak Position: la mejor posici\u00f3n que jam\u00e1s haya ocupado\n* Weeks on Chart: cu\u00e1ntas semanas ha estado en el top 100\n* top10_tag: si la posici\u00f3n ocupada estaba dentro del top 10\n* spotify_genre: g\u00e9nero personalizado de Spotify (lista por canci\u00f3n)\n* spotify_track_id: identificaci\u00f3n alfanum\u00e9rica\n* spotify_track_preview_url: versi\u00f3n preliminar breve de la URL de la pista\n* spotify_track_album \u2013 \u00c1lbum al que pertenece la canci\u00f3n\n* spotify_track_explicit: registro de etiquetas si la pista contiene lenguaje expl\u00edcito o no\n* spotify_track_duration_ms: duraci\u00f3n de la canci\u00f3n en milisegundos\n* spotify_track_popularity: qu\u00e9 tan popular es la pista en Spotify\n* danceability: m\u00e9trica que define la probabilidad de que la canci\u00f3n se pueda bailar\n* energy: M\u00e9trica que define qu\u00e9 tan en\u00e9rgica es la canci\u00f3n\n* key: tonalidad musical en la que se reproduce la canci\u00f3n\n* loudness: m\u00e9trica que define el volumen de la canci\u00f3n\n* mode: pregrabaci\u00f3n est\u00e9reo o mono\n* speechiness: M\u00e9trica que define cu\u00e1nto canto vs instrumento hay en la canci\u00f3n\n* acousticness: m\u00e9trica que define el grado en que se utilizan los instrumentos ac\u00fasticos.\n* instrumentalness: m\u00e9trica que define cu\u00e1nto de la canci\u00f3n utiliza instrumentos\n* liveness: m\u00e9trica que define el estado de \u00e1nimo de una canci\u00f3n\n* valence: M\u00e9trica que define la positividad de la canci\u00f3n\n* tempo: M\u00e9trica que define el tempo de la canci\u00f3n\n* time_signature: m\u00e9trica para etiquetar el tiempo de la canci\u00f3n"}, {"cell_type": "markdown", "metadata": {}, "source": "**Nombre completo del alumno:**"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "03acaa5cd7b3565e48aaacc1ea1aa615", "grade": false, "grade_id": "cell-81ef968c02ad077e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**INSTRUCCIONES**: en cada celda debes responder a la pregunta formulada, asegur\u00e1ndote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del c\u00e1lculo quede guardado exactamente en la variable que ven\u00eda inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). **No olvides borrar la l\u00ednea *raise NotImplementedError()* de cada celda cuando hayas completado la soluci\u00f3n de esa celda y quieras probarla**.\n\nDespu\u00e9s de cada celda evaluable ver\u00e1s una celda con c\u00f3digo. Ejec\u00fatala (no modifiques su c\u00f3digo) y te dir\u00e1 si tu soluci\u00f3n es correcta o no. Adem\u00e1s de esas pruebas, se realizar\u00e1n algunas m\u00e1s (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la soluci\u00f3n correcta o no. Aseg\u00farate de que, al menos, todas las celdas indican que el c\u00f3digo es correcto antes de enviar el notebook terminado.\n\n*No olvides todas las sentencias import necesarias. Una celda que no pueda ser ejecutada por faltar import no ser\u00e1 evaluada*"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "2ea346213656fa3f7adfbabc399b5423", "grade": false, "grade_id": "cell-b39d9a6714c9effd", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Sobre el dataset anterior (spotify_dataset.csv) se pide:"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "84b1bb462c457ed1bad0aefd634a9a75", "grade": false, "grade_id": "cell-41552ef3b7f22601", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 1 (1 punto)** Leerlo tratando de que Spark infiera el tipo de dato de cada columna, sin cachearlo.\n* Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente **si al leer le indicamos las siguientes opciones adicionales** adem\u00e1s de las que ya sueles usar: `.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")`.\n* Aseg\u00farate de que las **filas que no tienen el formato correcto sean descartadas**, indicando tambi\u00e9n la opci\u00f3n `mode` con el valor `DROPMALFORMED` como vimos en clase."}, {"cell_type": "code", "execution_count": 1, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "a991784a8c6305d691f5b3f7cebe1ebd", "grade": false, "grade_id": "read_csv", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\nimport pyspark.sql.functions as F\n\nspotify_df = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .option(\"quote\", \"\\\"\") \\\n    .option(\"escape\", \"\\\"\") \\\n    .option(\"mode\", \"DROPMALFORMED\") \\\n    .csv(\"gs://core-memento-378812/notebooks/jupyter/spotify_dataset.csv\")\n"}, {"cell_type": "code", "execution_count": 2, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "8675d8ca2c8833d9bb61bc265fd03a9b", "grade": true, "grade_id": "read_csv_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "assert(spotify_df.count() == 277965)\nassert(spotify_df.is_cached == False)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "e8a990e3c29ecb3fbe1989a71de142a0", "grade": false, "grade_id": "cell-284e8bf4964733e9", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 2 (2 puntos)** Limpiar y enriquecer las variables aplicando las siguientes transformaciones encadenadas a `spotify_df`:\n* Reemplazar la columna `WeekID` por su versi\u00f3n convertida a tipo fecha (DateType). Esto se har\u00e1 en una sola l\u00ednea:\n  * Primero se convierte en una columna de Timestamp  con `F.from_unixtime(F.unix_timestamp('nombreColumna', 'dd-MM-yyyy')`\n  * En esa misma l\u00ednea, dicha columna debe ser convertida a tipo DateType usando el m\u00e9todo de la clase Column visto en clase.\n* Crear una nueva columna `title_length` con la longitud (n\u00famero de caracteres) del t\u00edtulo de cada canci\u00f3n. PISTA: consulta la [documentaci\u00f3n oficial](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.length.html) de la funci\u00f3n `F.length`.\n* Reemplazar la columna `Performer` por el resultado de *cortar cada t\u00edtulo por el string* `\" & \"` (el car\u00e1cter & precedido y seguido de un espacio en blanco), lo cual te devolver\u00e1 una columna de vectores de string. PISTA: consulta la [documentaci\u00f3n oficial](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.split.html) de la funci\u00f3n `F.split`. En el caso de no haber ning\u00fan & el vector devuelto tendr\u00e1 1 elemento.\n* Crear una nueva columna `n_performers` con el n\u00famero de int\u00e9rpretes de una canci\u00f3n. Para ello, debes *contar el n\u00famero de elementos* de cada vector de la columna `Performer` y [la documentaci\u00f3n](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.size.html) de la funci\u00f3n `F.size` (ojo, no confundir con `length` utilizada antes). Ambas se pueden concatenar ya que act\u00faan sobre un objeto Column y devuelven otro objeto Column. No te preocupes por los elementos nulos que hubiese en esta columna.\n* A\u00f1adir dos columnas `year` y `month` que contengan respectivamente el a\u00f1o y el mes al que corresponde la fecha de la columna `WeekID`. PISTA: consulta la documentaci\u00f3n oficial de [la funci\u00f3n F.year](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.year.html) y de [la funci\u00f3n F.month](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.month.html). \n* Guardar el resultado de las tres transformaciones anteriores en una nueva variable `spotify_enriquecido_df` y **cachearla**.\n\n* Guardar en la variable `fila_mas_autores` el objeto Row de la canci\u00f3n con el **mayor n\u00famero de int\u00e9rpretes**, deshaciendo los posibles empates por la que tenga la **menor longitud en el t\u00edtulo** (cuidado: los dos criterios de ordenaci\u00f3n son independientes y contrapuestos. Recuerda las funciones `asc()` y `desc()` de los objetos columna). PISTA: utilizar la acci\u00f3n `first()` tras ordenar el dataframe `spotify_enriquecido_df`, el cual no est\u00e1 ni debe quedar ordenado, puesto que la versi\u00f3n ordenada de dicho dataframe no debe reemplazar a dicha variable, sino que puedes usar otra variable auxiliar para guardarla y aplicar `first()` sobre ella.\n* Accediendo a los campos del objeto anterior, almacenar en la variable `titulo_mas_autores` y `url_mas_autores` los campos del t\u00edtulo de la canci\u00f3n y su URL, respectivamente."}, {"cell_type": "code", "execution_count": 3, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "ebb59d02aa0d70c346d3564ecd3bf6e0", "grade": false, "grade_id": "enriquecer", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/11 16:45:33 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}], "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, asc,desc\n\nspotify_enriquecido_df = spotify_df.withColumn(\"WeekID\", F.to_date(F.from_unixtime(F.unix_timestamp('WeekID', 'dd-MM-yyyy'), 'yyyy-MM-dd'))) \\\n                                   .withColumn(\"title_length\", F.length(col(\"song\"))) \\\n                                   .withColumn(\"Performer\", F.split(col(\"Performer\"), ' & ')) \\\n                                   .withColumn(\"n_performers\", F.size(col(\"Performer\"))) \\\n                                   .withColumn(\"year\", F.year(\"WeekID\")).withColumn(\"month\", F.month(\"WeekID\"))\n\n\nspotify_enriquecido_ordenado_df = spotify_enriquecido_df.orderBy(desc(\"n_performers\"), asc(\"title_length\"))\nspotify_enriquecido_df.cache()\n\nfila_mas_autores = spotify_enriquecido_ordenado_df.first()\n\ntitulo_mas_autores = fila_mas_autores[\"Song\"]\nurl_mas_autores = fila_mas_autores[\"url\"]\n"}, {"cell_type": "code", "execution_count": 4, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "883f3613fea47162af11b0d585e24ca7", "grade": true, "grade_id": "enriquecer_test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "tipos = dict(spotify_enriquecido_df.dtypes)\nassert(tipos[\"title_length\"] == \"int\" and \n       tipos[\"Performer\"] == \"array<string>\" and \n       tipos[\"n_performers\"] == \"int\")\nassert(sum([1 for x in [\"title_length\", \"n_performers\", \"year\", \"month\"] if x in spotify_enriquecido_df.columns]) == 4)\nassert(fila_mas_autores.n_performers == 3)\nassert(fila_mas_autores.url == \"http://www.billboard.com/charts/hot-100/1998-08-01\")\nr = spotify_enriquecido_df.select(F.mean(\"n_performers\").alias(\"n_performers\"),\n                                  F.mean(\"title_length\").alias(\"title_length\")).first()\nassert(round(r.n_performers, 2) == 1.07)\nassert(round(r.title_length, 2) == 15.59)\n"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "a28dfeaf292d1cdca19dcd73dbbe9dee", "grade": false, "grade_id": "cell-58e421fdc7111d32", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 3 (1.5 puntos)** Partiendo del DataFrame `spotify_enriquecido_df` se pide:\n\n* Crear un nuevo DataFrame que tenga tantas filas como a\u00f1os distintos aparecen, y tantas columnas como meses del a\u00f1o m\u00e1s una (que es el a\u00f1o y debe estar a la izquierda del todo). Las columnas deben aparecer de izqda a dcha del 1 a 12 -recuerda que el m\u00e9todo `select()` te las devuelve en el orden que se las pidas-. En cada casilla de dicho DF debe aparecer el *n\u00famero de canciones* ***distintas*** que han estado durante ese mes en alg\u00fan momento en el Top 10. Se deben contar canciones distintas atendiendo a la columna `SongID`.\n  * PISTA: el punto de partida debe ser el DF formado solamente por aquellas semanas que est\u00e1n en el top10. A partir de ah\u00ed debes construir la tabla anterior. Recuerda qu\u00e9 funci\u00f3n de `pyspark.sql.functions` te sirve para contar el n\u00famero de elementos **distintos** de una columna o en este caso, de un grupo.\n  * PISTA: examina los valores posibles de la columna `top10_tag` para deducir con cu\u00e1les debes quedarte como punto de partida.\n* Rellenar los valores nulos del DataFrame resultante con 0.\n* Tras rellenarlos, a\u00f1adir una nueva columna `n_top10_promedio` que contenga para cada a\u00f1o (fila) el n\u00famero medio (redondeado a 2 cifras decimales con la funci\u00f3n `F.round`) de canciones distintas que hay en todos los meses de ese a\u00f1o. \n  * PISTA: esta operaci\u00f3n se realiza exclusivamente mediante operaciones aritm\u00e9ticas con columnas. No se necesita F.mean ni F.sum ni nada parecido. La funci\u00f3n `F.round` se debe aplicar al objeto columna resultante de la operaci\u00f3n aritm\u00e9tica.\n* Las filas del DataFrame resultante deben quedar ordenadas de menor a mayor a\u00f1o.\n* Guardar el resultado de las transformaciones anteriores en una nueva variable `distintos_top10_df."}, {"cell_type": "code", "execution_count": 5, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "0417d7385ba6c8b964d5c79513f92083", "grade": false, "grade_id": "anios_top10", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.sql.functions import countDistinct, round, count, year, month\n\ndistintos_top10_df = (\n    spotify_enriquecido_df \n    .filter(col('top10_tag') == 'Top 10')\n    .groupBy(year('WeekID').alias('year'), month('WeekID').alias('month'))\n    .agg(countDistinct('SongID').alias('num_canciones'))\n    .cache()\n)\n\nyears = distintos_top10_df.select('year').distinct().orderBy('year')\nmonths_cols = [str(i) for i in range(1, 13)]\ncolumns = ['year'] + months_cols\n\ndistintos_top10_df =distintos_top10_df.groupBy('year').pivot('month', months_cols).sum('num_canciones') \\\n    .orderBy('year')\n\n\ndistintos_top10_df = years.join(distintos_top10_df, on='year', how='left').fillna(0)\n\ndistintos_top10_df = distintos_top10_df.withColumn(\n    'n_top10_promedio',\n    round((col('1') + col('2') + col('3') + col('4') + col('5') + col('6') +\n           col('7') + col('8') + col('9') + col('10') + col('11') + col('12')) / 12, 2)\n)\n\ndistintos_top10_df =distintos_top10_df.orderBy(col('year'))\n\n"}, {"cell_type": "code", "execution_count": 6, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "64caf53cd93779a62d4fe863136b6394", "grade": true, "grade_id": "anios_top10_test", "locked": true, "points": 1.5, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "lista = distintos_top10_df.take(5)\nassert(lista[0].year == 1958 and lista[0][\"12\"] == 12 and lista[0].n_top10_promedio == 9.42)\nassert(lista[2].year == 1960 and lista[2][\"4\"] == 21 and lista[2].n_top10_promedio == 24.58)"}, {"cell_type": "markdown", "metadata": {}, "source": "**Ejercicio 4 (1.5 puntos)** Partiendo de nuevo del DataFrame almacenado en `spotify_enriquecido_df` se pide:\n\n* Reemplazar la columna `top10_tag`, que actualmente son \"Top 10\" y \"Other\" (literalmente) por una versi\u00f3n recategorizada (usando `F.when`) cuyos valores sean los *n\u00fameros enteros* 1 y 0, respectivamente (es decir, Top 10 -> 1 y Other -> 0). No es necesario usar F.lit, aunque puedes hacerlo.\n* Crear una nueva columna `semanas_top10` que contenga para cada canci\u00f3n el n\u00famero **total** de semanas que ha estado en el Top 10 **en ese mismo a\u00f1o** al que corresponde la fila. Est\u00e1 prohibido utilizar JOIN. Debe resolverse obligatoriamente creando una ventana en la  variable `w` (ATENCI\u00d3N: hay que contar para **cada canci\u00f3n** y **ese mismo a\u00f1o**).\n  * PISTA: vas a necesitar la columna `top10_tag` que has recategorizado como enteros.\n  * PISTA: recuerda importar la funci\u00f3n Window de `pyspark.sql`.\n* A continuaci\u00f3n, seleccionar las columnas `SongID`, `Performer` y `semanas_top10` y quitar duplicados.\n* Ordenar el DataFrame resultante descentemente en base a `semanas_top10` y desempatar descendentemente en base a `SongID` (Spark utilizar\u00e1 orden alfab\u00e9tico al ser de tipo string).\n* Guardar el resultado de las transformaciones anteriores en la variable `semanas_top10_df`."}, {"cell_type": "code", "execution_count": 7, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "11e732ab7b3af13d17daea6ddc131e32", "grade": false, "grade_id": "pegar_top10", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf_recategorizado = spotify_enriquecido_df.withColumn(\"top10_tag_recategorizado\",\n                                                     F.when(F.col(\"top10_tag\") == \"Top 10\", 1)\n                                                     .otherwise(0))\n\nw = Window.partitionBy(\"SongID\", F.year(\"WeekID\"))\n\ndf_con_semanas_top10 = df_recategorizado.withColumn(\"semanas_top10\",\n                                                    F.sum(\"top10_tag_recategorizado\").over(w))\n\ndf_resultado = df_con_semanas_top10.select(\"SongID\", \"Performer\", \"semanas_top10\").distinct().dropDuplicates()\n\ndf_resultado_ordenado = df_resultado.orderBy(F.desc(\"semanas_top10\"), F.desc(\"SongID\"))\nsemanas_top10_df = df_resultado_ordenado\n\n"}, {"cell_type": "code", "execution_count": 8, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "c89515020e46b13a08c0a62dff0a6f9d", "grade": true, "grade_id": "pegar_top10_test", "locked": true, "points": 1.5, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "lista = semanas_top10_df.take(4)\nassert(lista[0].semanas_top10 == 30 and \"Bad Guy\" in lista[0].SongID)\nassert(lista[1].semanas_top10 == 28)\nassert(lista[2].semanas_top10 == 28)\nassert(lista[3].semanas_top10 == 27 and \"Girls Like You\" in lista[3].SongID)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "ab04e483657295cb6047f3a0a5e8af94", "grade": false, "grade_id": "cell-67b8088c6544f3a3", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 5 (1 punto)** Partiendo del DataFrame almacenado en `semanas_top10_df`:\n\n* Reemplazar la columna `Performer` por el resultado de *explotarla* utilizando la [funci\u00f3n](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html) `F.explode(...)`, esto es,  desenrollar los vectores que hay en dicha columna, de manera que cada valor del vector pase a tener su propia fila completa. Por ejemplo si una casilla de esa columna contiene el vector \\[\"Beyonce\", \"Pitbull\"\\] porque hayan colaborado juntos en alguna canci\u00f3n, la operaci\u00f3n `F.explode` sobre esa columna dar\u00e1 lugar a dos filas en el DataFrame resultante, que ser\u00e1n id\u00e9nticas entre s\u00ed en todas las columnas excepto en la columna `Performer`, donde una de las dos filas tendr\u00e1 el valor \"Beyonce\" y la otra fila tendr\u00e1 el valor \"Pitbull\". El DataFrame resultante tendr\u00e1 probablemente bastantes m\u00e1s filas que el original.\n* A continuaci\u00f3n, encadenando una transformaci\u00f3n aplicada al DF devuelto por el apartado anterior, crear un nuevo DataFrame con tantas filas como int\u00e9rpretes distintos y dos columnas, que sean el nombre dint\u00e9rprete y el n\u00famero **total** de semanas que sus canciones han estado en el top 10, llamando a dicha columna `total_semanas_top10` (poniendo sobre la marcha en el momento de crearla).\n* Ordenar dicho DataFrame descendentemente en base a `total_semanas_top10`, y guardar el resultado en una variable `top_performers_df`\n* Utilizar una acci\u00f3n que lleve al Driver, a la variable `top_performers`, una lista de Python con el top 3 de estos artistas. \u00bfTe suenan...?"}, {"cell_type": "code", "execution_count": 9, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "ece63cff5860a2fcd2fca717971eafa4", "grade": false, "grade_id": "mejores_artistas", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------+-------------------+\n|         Performer|total_semanas_top10|\n+------------------+-------------------+\n|         Glee Cast|                178|\n|             Drake|                 97|\n|      Taylor Swift|                 91|\n|        Elton John|                 66|\n|       The Beatles|                 65|\n|           Madonna|                 64|\n|   Aretha Franklin|                 60|\n|     Stevie Wonder|                 59|\n|The Rolling Stones|                 57|\n|       Marvin Gaye|                 55|\n|        Diana Ross|                 55|\n|    The Beach Boys|                 52|\n|              Cher|                 51|\n|         Lil Wayne|                 50|\n|           Chicago|                 50|\n|   Michael Jackson|                 50|\n|      Neil Diamond|                 49|\n|            Future|                 49|\n|   The Temptations|                 49|\n|    Dionne Warwick|                 48|\n+------------------+-------------------+\nonly showing top 20 rows\n\n+--------------------+--------------------+-------------+\n|              SongID|           Performer|semanas_top10|\n+--------------------+--------------------+-------------+\n|Bad GuyBillie Eilish|       Billie Eilish|           30|\n|Uptown Funk!Mark ...|Mark Ronson Featu...|           28|\n|That's What I Lik...|          Bruno Mars|           28|\n|Girls Like YouMar...|Maroon 5 Featurin...|           27|\n|Party Rock Anthem...|LMFAO Featuring L...|           26|\n|Party Rock Anthem...|            GoonRock|           26|\n|Old Town RoadLil ...|Lil Nas X Featuri...|           26|\n|     God's PlanDrake|               Drake|           26|\n|Truly Madly Deepl...|       Savage Garden|           25|\n| Trap QueenFetty Wap|           Fetty Wap|           25|\n|Lucid DreamsJuice...|          Juice WRLD|           25|\n|DespacitoLuis Fon...|          Luis Fonsi|           25|\n|DespacitoLuis Fon...|Daddy Yankee Feat...|           25|\n|Yeah!Usher Featur...|Usher Featuring L...|           24|\n|Yeah!Usher Featur...|            Ludacris|           24|\n|     Wow.Post Malone|         Post Malone|           24|\n|Somebody That I U...|Gotye Featuring K...|           24|\n|You're Still The ...|        Shania Twain|           23|\n|We Belong Togethe...|        Mariah Carey|           23|\n|       Too CloseNext|                Next|           23|\n+--------------------+--------------------+-------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql.functions import explode\n\nexploded_df = semanas_top10_df.withColumn(\"Performer\", F.explode(semanas_top10_df.Performer))\ntop_performers_df = exploded_df.groupBy(\"Performer\").agg(F.count(\"*\").alias(\"total_semanas_top10\")).orderBy(\"total_semanas_top10\", ascending=False)\n\n\ntop_performers = [row[0] for row in top_performers_df.select(\"Performer\").take(3)]\n\ntop_performers_df.show()\nexploded_df.show()"}, {"cell_type": "code", "execution_count": 10, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "f5d764c10da963b9f70a5c693e18f653", "grade": true, "grade_id": "mejores_artistas_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"ename": "AttributeError", "evalue": "'str' object has no attribute 'Performer'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(top_performers_df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m7992\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[43mtop_performers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPerformer\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMariah Carey\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m top_performers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtotal_semanas_top10 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m208\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(top_performers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mPerformer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMadonna\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m top_performers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtotal_semanas_top10 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m205\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(top_performers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mPerformer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Beatles\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m top_performers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtotal_semanas_top10 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m182\u001b[39m)\n", "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'Performer'"]}], "source": "assert(top_performers_df.count() == 7992)\nassert(top_performers[0].Performer == \"Mariah Carey\" and top_performers[0].total_semanas_top10 == 208)\nassert(top_performers[1].Performer == \"Madonna\" and top_performers[1].total_semanas_top10 == 205)\nassert(top_performers[2].Performer == \"The Beatles\" and top_performers[2].total_semanas_top10 == 182)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "6b766faeca84b2f8e45fc02fc2b19ce3", "grade": false, "grade_id": "intro_clustering", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Este ejercicio y los dos siguientes plantean un algoritmo de **clustering (K-means)** sobre las canciones, bas\u00e1ndonos solamente en los atributos num\u00e9ricos que describen los par\u00e1metros de cada canci\u00f3n. Concretamente, usaremos solamente las siguientes 10 columnas:\n\n* danceability: m\u00e9trica que define la probabilidad de que la canci\u00f3n se pueda bailar\n* energy: M\u00e9trica que define qu\u00e9 tan en\u00e9rgica es la canci\u00f3n\n* loudness: m\u00e9trica que define el volumen de la canci\u00f3n\n* mode: pregrabaci\u00f3n est\u00e9reo o mono (tiene dos valores: 0 y 1)\n* speechiness: M\u00e9trica que define cu\u00e1nto canto vs instrumento hay en la canci\u00f3n\n* acousticness: m\u00e9trica que define el grado en que se utilizan los instrumentos ac\u00fasticos.\n* instrumentalness: m\u00e9trica que define cu\u00e1nto de la canci\u00f3n utiliza instrumentos\n* liveness: m\u00e9trica que define el estado de \u00e1nimo de una canci\u00f3n\n* valence: M\u00e9trica que define la positividad de la canci\u00f3n\n* tempo: M\u00e9trica que define el tempo de la canci\u00f3n\n\nPuesto que K-means con distancia eucl\u00eddea requiere que todas las variables est\u00e9n aproximadamente en el mismo rango para que todas influyan por igual en la distancia, primero vamos a reescalar todas las variables anteriores mediante un MinMaxScaler de Spark. Eso significa que tras el reescalado, todas tendr\u00e1n valores entre 0 y 1. Como el MinMaxScaler requiere una columna de tipo vector como entrada porque es capaz de reescalar de una sola vez muchas features, construiremos **antes del reescalado** una columna de tipo vector con las features originales. La salida del reescalado sigue siendo una columna de tipo vector, tal como exige el algoritmo K-means que entrenaremos despu\u00e9s."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "750c6adc044669e0f77d9ec25f90a3b3", "grade": false, "grade_id": "cell-9065e931deeb24cc", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 6 (1 punto)** Partiendo del DataFrame `spotify_enriquecido_df`, tal como fue le\u00eddo del fichero CSV, se pide:\n\n* Ejecutar el c\u00f3digo dado para crear el DataFrame `spotify_numeric_df`. No debes resolver nada en este momento, sino dejar este c\u00f3digo sin modificar.\n* Crear en la variable `vector_assembler` un VectorAssembler que reciba como entradas (argumento `inputCols`) la lista `cluster_vars` que ya viene creada, y como salida cree una nueva columna llamada `feats_vectorizadas`.\n* Crear en la variable `minmax_scaler` un [objeto `MinMaxScaler()` de Spark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinMaxScaler.html) que est\u00e9 configurado para utilizar como entrada la columna `feats_vectorizadas`, y que cree una nueva columna de tipo vector llamada `feats_reescaladas`.\n  * **NOTA IMPORTANTE**:  `MinMaxScaler` recibe como \u00fanica columna de entrada una columna de tipo vector, ya que es capaz de re-escalar de una sola vez (y de manera independiente) un conjunto de columnas (o mejor dicho, una columna de tipo vector que en realidad representa varias columnas independientes ya colapsadas). Por eso en este caso particular, la pen\u00faltima pieza del pipeline, que va justo antes del algoritmo K-Means, **NO** va a ser el `vector_assembler` como suele ser habitual, sino el `minmax_scaler`, y por eso el `vector_assembler` debe ir antes de dicha pieza, ya que es quien va a crear la columna de vectores que despu\u00e9s recibir\u00e1 el `minmax_scaler` como entrada.\n  * NOTA: los par\u00e1metros de valor m\u00ednimo y m\u00e1ximo que debemos indicar al MinMaxScaler como extremos de la columna que va a crear nueva, vienen ya fijados por defecto a 0 y 1 respectivamente (es decir, cada columna se va a re-escalar, dando lugar a otra nueva que va entre 0 y 1). Esto es justo lo que queremos para K-means, as\u00ed que los dejamos sin especificar para que tomen esos valores.\n* Crear en la variable `kmeans`un [estimador K-Means](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html) con argumento `k=10` clusters, que reciba como entrada la columna `feats_reescaladas` creada en la etapa del vector assembler, y genere como salida (argumento `predictionCol`) una nueva columna llamada `cluster`, que haga como m\u00e1ximo 200 iteraciones (argumento `maxIter`) y que tenga semilla 12345 (argumento `seed`, imprescindible para que coincidan los resultados de los tests).\n  * A priori desconocemos el n\u00famero de clusters que necesitamos, que no tiene por qu\u00e9 coincidir con el n\u00famero de g\u00e9neros musicales ya que tambi\u00e9n estamos juntando datos de muchos a\u00f1os diferentes y esto tambi\u00e9n influye. Deber\u00edamos hacer un an\u00e1lisis con regla del codo y tratar de interpretar los clusters para varios valores de k distintos hasta quedar contentos con un valor de k, pero para simplificar, omitiremos este proceso."}, {"cell_type": "code", "execution_count": 11, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "63807dcffc68d658c598bbf100f977c1", "grade": false, "grade_id": "piezas", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import MinMaxScaler, VectorAssembler   # recuerda los imports necesarios para MinMaxScaler y VectorAssembler\nfrom pyspark.ml.clustering import KMeans           # recuerda el import necesario para el algoritmo KMeans\n\ncluster_vars = [\"danceability\", \"energy\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \"instrumentalness\", \n                \"liveness\", \"valence\", \"tempo\"]\n\nspotify_numeric_df = spotify_enriquecido_df\\\n    .groupBy(\"SongID\").agg(*[F.first(c).alias(c) for c in [\"year\", \"spotify_track_duration_ms\"] + cluster_vars])\\\n    .distinct()\\\n    .cache()\n\nminmax_scaler = MinMaxScaler(\n    inputCol=\"feats_vectorizadas\",\n    outputCol=\"feats_reescaladas\"\n)\n\nvector_assembler = VectorAssembler(\n    inputCols=cluster_vars,\n    outputCol=\"feats_vectorizadas\"\n)\nkmeans = KMeans(\n    k=10,\n    featuresCol=\"feats_reescaladas\",\n    predictionCol=\"cluster\",\n    maxIter=200,\n    seed=12345\n)\n"}, {"cell_type": "code", "execution_count": 12, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "8786c082c39ebf4b99d50d4cc2bd0b0f", "grade": true, "grade_id": "piezas_test", "locked": true, "points": 1.5, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "assert(spotify_numeric_df.count() == 23751)\nassert(len(spotify_numeric_df.columns) == 13)\nassert(minmax_scaler.getInputCol() == \"feats_vectorizadas\")\nassert(minmax_scaler.getOutputCol() == \"feats_reescaladas\")\nassert(vector_assembler.getInputCols() == cluster_vars)\nassert(kmeans.getK() == 10 and kmeans.getFeaturesCol() == \"feats_reescaladas\" \n       and kmeans.getPredictionCol() == \"cluster\" and kmeans.getMaxIter() == 200\n       and kmeans.getSeed() == 12345)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "a6ba1db0081758452a16b1a0e84ac109", "grade": false, "grade_id": "cell-7938478156b24ff2", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 7 (2 puntos)** \n* Crear un pipeline en la variable `pipeline` que tenga las tres etapas anteriores (el vector_assembler, el minmax_scaler y el kmeans) por ese orden.\n* Entrenar el pipeline sobre el DF `spotify_numeric_df` y guardar el resultado (pipeline entrenado) en la variable `pipeline_model`.\n* Aplicar el pipeline entrenado para transformar el DataFrame `spotify_numeric_df` y guardar el resultado en la variable `clustered_songs_df`, que debe quedarse **cacheada**.\n* Analizar el resultado obtenido mostrando m\u00e9tricas agregadas de las variables dentro de cada cluster. Para ello, a partir de `clustered_songs_df`, crear en la variable `metricas_clusters_pd` un nuevo DataFrame con tantas filas como clusters (10) y que tenga 13 columnas que ser\u00e1n:\n  * La columna m\u00e1s a la izquierda el cluster al que corresponde la fila, que es por lo que estamos agrupando.\n  * La segunda ser\u00e1 la mediana de la columna `year` en cada grupo. Dicha columna no se utiliz\u00f3 para el clustering pero existe en el DF `clustered_songs_df` \n  * Las siguientes 10 columnas deben ser la mediana de cada una de las 10 variables num\u00e9ricas utilizadas para el cluster (excepto para \"mode\" que debemos usar la media y no la mediana)\n  * La \u00faltima columna debe ser el recuento del n\u00famero de filas de cada grupo, que debe renombrarse como `n`. \n* Tras el c\u00e1lculo, el DataFrame resultante debe pasarse a DataFrame de **pandas**, y eso es lo que debe guardarse en `metricas_clusters_pd`. \n  * Se debe usar la **mediana** de cada columna en cada grupo. Puesto que la mediana no est\u00e1 disponible en la API de columnas en Spark 2.4 (se incluy\u00f3 en versiones posteriores) pero s\u00ed que est\u00e1 disponible en la API de SQL puro, tenemos que utilizar para cada agregaci\u00f3n la expresi\u00f3n `F.expr(\"percentile_approx(nombreColumna, 0.5)\").alias(nombreColumna)` que devuelve un objeto Column con la agregaci\u00f3n y ya renombrado para llamarse igual que la columna a la que le estamos aplicando la mediana. \n  * En la columna `mode` se debe usar la **media** habitual, no la mediana, tambi\u00e9n renombrada para que se llame \"mode\" tras aplicar la agregaci\u00f3n.\n  * NOTA: estamos haciendo clustering, con lo que no hay ninguna divisi\u00f3n en train y test ni nada parecido! :-)"}, {"cell_type": "code", "execution_count": 13, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "a5d7a7dfb0fb8cf58a379e7d3032a13e", "grade": false, "grade_id": "salidas_clusters", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/11 16:47:26 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n23/05/11 16:47:26 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n                                                                                \r"}], "source": "from pyspark.ml import Pipeline\n\n\npipeline = Pipeline(stages=[vector_assembler, minmax_scaler, kmeans])\n\npipeline_model = pipeline.fit(spotify_numeric_df)\n\nclustered_songs_df = pipeline_model.transform(spotify_numeric_df).cache()\n\nmetricas_clusters_df = clustered_songs_df.groupBy(\"cluster\").agg(\n    F.expr(\"percentile_approx(year, 0.5)\").alias(\"year\"),\n    *[F.expr(\"percentile_approx({}, 0.5)\".format(col)).alias(col) for col in cluster_vars if col != \"mode\"],\n    F.mean(\"mode\").alias(\"mode\"),\n    F.count(\"*\").alias(\"n\")\n)\n\nmetricas_clusters_pd = metricas_clusters_df.toPandas()\n"}, {"cell_type": "code", "execution_count": 15, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "9299cdb0856ecd55483bd16eeeadf2a3", "grade": true, "grade_id": "salidas_clusters_test", "locked": true, "points": 1.5, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(metricas_clusters_pd.shape == (10, 13))\ncolumnas = [\"cluster\", \"year\", \"n\"] + cluster_vars   # todas estas columnas tienen que existir en metricas_clusters_pd\nassert(sum([1 for x in columnas if x in metricas_clusters_pd.columns]) == 13)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}